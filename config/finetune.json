{
    "tokenizer": {
        "pretrained_model_name_or_path": "kevin510/friday",
        "model_max_length": 2048,
        "padding_side": "right",
        "use_fast": true,
        "trust_remote_code": true
    },
    "model": {
        "pretrained_model_name_or_path": "microsoft/Phi-4-mini-reasoning",
        "device_map": "cuda",
        "attn_implementation": "flash_attention_2",
        "cfg_vision_tower": {
            "pretrained_model_name_or_path": "kevin510/fast-vit-hd",
            "type": "fastvit",
            "s2_scales": "512,1024",
            "use_s2": true,
            "pad_to_square": true,
            "model_params": { "device_map": "cuda", "trust_remote_code": true }
        },
        "cfg_vision_adapter": {
            "input_dim": 6144,
            "hidden_dim": 3072,
            "output_dim": 3072,
            "num_layers": 2,
            "activation": "gelu",
            "device": "cuda",
            "checkpoint_path": "model/mm_projector.bin"
        }
    },
    "data": {
        "dataset_type": "finetuning",
        "data_path": "datasets/llava_v1_5_mix665k/llava_v1_5_mix665k_filtered.json",
        "image_dir": "datasets/llava_v1_5_mix665k"
    },
    "training": {
        "mpt_attn_impl": "triton",
        "group_by_modality_length": true,
        "save_vision_adapter": true,
        "save_language_model": true,
        "freeze_language_model": true,
        "freeze_vision_tower": true,
        "freeze_vision_adapter": false,
        "vision_adapter_lr": 2e-5,
        "lora_enable": true,
        "lora_params": {
            "r": 128,
            "lora_alpha": 256,
            "lora_dropout": 0.05,
            "bias": "none"
        },

        
        "local_rank": -1,
        "remove_unused_columns": false,
        "optim": "adamw_torch",
        "fp16": false,
        "bf16": true,
        "tf32": true,
        "eval_strategy": "no",
        "output_dir": "./checkpoints-finetune/friday",
        "num_train_epochs": 1,
        "per_device_train_batch_size": 14,
        "gradient_accumulation_steps": 2,
        "save_strategy": "steps",
        "save_steps": 1000,
        "save_total_limit": 1,
        "learning_rate": 2e-4,
        "weight_decay": 0.0,
        "warmup_ratio": 0.03,
        "lr_scheduler_type": "cosine",
        "logging_steps": 1,
        "gradient_checkpointing": true,
        "dataloader_num_workers": 4,
        "max_grad_norm": 1.0,
        "report_to": "wandb",
        "deepspeed": {
            "fp16": {
                "enabled": "auto",
                "loss_scale": 0,
                "loss_scale_window": 1000,
                "initial_scale_power": 16,
                "hysteresis": 2,
                "min_loss_scale": 1
            },
            "bf16": {
                "enabled": "auto"
            },
            "train_micro_batch_size_per_gpu": "auto",
            "train_batch_size": "auto",
            "gradient_accumulation_steps": "auto",
            "zero_optimization": {
                "stage": 2,
                "overlap_comm": true,
                "contiguous_gradients": true,
                "sub_group_size": 1e9,
                "reduce_bucket_size": "auto"
            }
        }
    }
  }
  