https://github.com/aimagelab/LLaVA-MORE


Vision Encoder:
	https://huggingface.co/google/siglip2-base-patch16-224
	SigLIP2 ViT-L/14
	Scaling on Scales (S^2) [53]
Adapter:
	2 layer MLP:
        Linear()
        Gelu()
        Linear()
LLM:
	https://huggingface.co/microsoft/Phi-4-mini-instruct
Datasets:
	https://github.com/aimagelab/LLaVA-MORE/blob/main/docs/Data.md
	Pretrain:
		https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K
	Postrain:
		https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K
		Maybe:
			append VQAv2 (âˆ¼200 K Q-A pairs) so the model learns to answer concisely when required
			GQA, OK-VQA, TextVQA	Add small slices if you want spatial, commonsense or OCR coverage later.
			
Evaluations:
	https://github.com/EvolvingLMMs-Lab/lmms-eval
	Text-VQA
	Science-QA
	AI2DD
	SEED-vid
	SEED-all
	SEED-img
	MMMU
	MMBench-Cn
	MMBench-En
	POPE
	GQA
	MME-P
	MME-C


Add multi-lingual support and vision?
SONAR for audio?
https://arxiv.org/abs/2503.01743

Questions:
	which layer to pull features from vision encoder? -2? Last layer and -2 are the same shape.


s2:
	input # B X 3 X 384 x 384
	input_size=384
	s2_scales=[384,768]
	scaled_features=[]
	for s2_scale in s2_scales:
		num_splits=input_size // s2_scale 
		splits=split( resize(input, s2_scale), num_splits) 		# (B*num_splits**2) x 3 x 384 x 384
		scaled_features.append(
			vision_tower(splits) 							# (B*num_splits**2) x P x P x E
		)
	
	out_features=merge(scaled_features) 						# B x num_splits*P x num_splits*P x E
	out_features=pool(out_features)								# B x P x P x E
	out_features=concat(features, vision_tower(input))			# B x P x P x 2*E
		

AnyRes vs S2 vs Both?
			
